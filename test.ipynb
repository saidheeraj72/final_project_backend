{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Torch 2.6.0 | device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, glob, random, csv, shutil, time\n",
    "from pathlib import Path\n",
    "import cv2, torch, torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from ultralytics import YOLO\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('‚úÖ Torch', torch.__version__, '| device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW images  : /Users/saidheeraj/Desktop/Bot/filtered_cattle_dataset\n",
      "Crop output : /Users/saidheeraj/Desktop/Bot/crops\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ROOT         = Path().resolve()             # notebook folder\n",
    "RAW_DIR      = ROOT/'filtered_cattle_dataset'               # ‚â° cattle_1/ ‚Ä¶ cattle_20/\n",
    "DET_WEIGHTS  = '/Users/saidheeraj/Desktop/Yolo_cutom/muzzle_detector_v3.pt'    # muzzle detector you already trained\n",
    "CROP_DIR     = ROOT/'crops'                 # will be created\n",
    "REC_WT       = ROOT/'weights'/'rec_model.pth'\n",
    "\n",
    "DET_CONF   = 0.35\n",
    "BATCH      = 32\n",
    "EPOCHS     = 20\n",
    "LR         = 1e-4\n",
    "\n",
    "CROP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "print('RAW images  :', RAW_DIR)\n",
    "print('Crop output :', CROP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv8 detector ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f73797f64345c0b723e0c1d476cf40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Folders:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cropping done ‚Äì crops stored in /Users/saidheeraj/Desktop/Bot/crops\n"
     ]
    }
   ],
   "source": [
    "print('Loading YOLOv8 detector ...')\n",
    "detector = YOLO(str(DET_WEIGHTS))  # ultralytics YOLO loader\n",
    "\n",
    "# loop over every cattle folder\n",
    "for cow_dir in tqdm(sorted(RAW_DIR.iterdir()), desc='Folders'):\n",
    "    if not cow_dir.is_dir():\n",
    "        continue\n",
    "    cow_id = cow_dir.name\n",
    "    out_dir = CROP_DIR / cow_id\n",
    "    out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for img_path in cow_dir.glob('*'):\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            print('‚ùå cannot read', img_path)\n",
    "            continue\n",
    "\n",
    "        # BGR ‚ûî RGB\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Run detection\n",
    "        results = detector.predict(img_rgb, device=DEVICE, conf=DET_CONF, verbose=False)\n",
    "\n",
    "        # results[0].boxes.xyxy ‚Üí bounding boxes\n",
    "        boxes = results[0].boxes.xyxy.cpu().numpy()  # (x1, y1, x2, y2)\n",
    "\n",
    "        for i, (x1, y1, x2, y2) in enumerate(boxes):\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "            if crop.size == 0 or min(crop.shape[:2]) < 20:\n",
    "                continue\n",
    "            cv2.imwrite(str(out_dir / f'{img_path.stem}_{i}.jpg'), crop)\n",
    "\n",
    "print('‚úÖ Cropping done ‚Äì crops stored in', CROP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20  val_acc=0.100\n",
      "Epoch 02/20  val_acc=0.100\n",
      "Epoch 03/20  val_acc=0.500\n",
      "Epoch 04/20  val_acc=0.600\n",
      "Epoch 05/20  val_acc=0.600\n",
      "Epoch 06/20  val_acc=0.900\n",
      "Epoch 07/20  val_acc=1.000\n",
      "Epoch 08/20  val_acc=0.900\n",
      "Epoch 09/20  val_acc=1.000\n",
      "Epoch 10/20  val_acc=1.000\n",
      "Epoch 11/20  val_acc=1.000\n",
      "Epoch 12/20  val_acc=1.000\n",
      "Epoch 13/20  val_acc=1.000\n",
      "Epoch 14/20  val_acc=1.000\n",
      "Epoch 15/20  val_acc=1.000\n",
      "Epoch 16/20  val_acc=1.000\n",
      "Epoch 17/20  val_acc=1.000\n",
      "Epoch 18/20  val_acc=1.000\n",
      "Epoch 19/20  val_acc=1.000\n",
      "Epoch 20/20  val_acc=1.000\n",
      "üèÅ Training complete ‚Ä¢ Best validation accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Define transforms\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Dataset class\n",
    "class CropDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, tfm):\n",
    "        self.samples = []\n",
    "        for cls_name in sorted(os.listdir(root)):\n",
    "            for p in glob.glob(f'{root}/{cls_name}/*.jpg'):\n",
    "                self.samples.append((p, cls_name))\n",
    "        random.shuffle(self.samples)\n",
    "        self.tfm = tfm\n",
    "        self.labels = sorted({l for _, l in self.samples})\n",
    "        self.l2i = {l:i for i,l in enumerate(self.labels)}\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p, lbl = self.samples[idx]\n",
    "        img = Image.open(p).convert('RGB')\n",
    "        return self.tfm(img), self.l2i[lbl]\n",
    "\n",
    "# Load dataset\n",
    "full_ds = CropDS(CROP_DIR, train_tf)\n",
    "val_len = int(0.1 * len(full_ds))\n",
    "train_ds, val_ds = torch.utils.data.random_split(full_ds, [len(full_ds) - val_len, val_len])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=0)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "\n",
    "# Define model\n",
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "model.fc = nn.Linear(model.fc.in_features, len(full_ds.labels))\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt  = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Train\n",
    "best = 0\n",
    "for ep in range(EPOCHS):\n",
    "    model.train()\n",
    "    for x, y in train_dl:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        loss = crit(model(x), y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    corr = tot = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_dl:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            pred = model(x).argmax(1)\n",
    "            corr += (pred == y).sum().item()\n",
    "            tot += y.size(0)\n",
    "    acc = corr / tot\n",
    "    print(f'Epoch {ep+1:02d}/{EPOCHS}  val_acc={acc:.3f}')\n",
    "\n",
    "    if acc > best:\n",
    "        best = acc\n",
    "        REC_WT.parent.mkdir(exist_ok=True, parents=True)\n",
    "        torch.save({'model': model.state_dict(), 'classes': full_ds.labels}, REC_WT)\n",
    "\n",
    "print('üèÅ Training complete ‚Ä¢ Best validation accuracy:', best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo image: /Users/saidheeraj/Desktop/Bot/filtered_cattle_dataset/cattle_9/IMG_20241108_094253.jpg\n",
      "üñºÔ∏è Annotated image saved at ‚Üí /Users/saidheeraj/Desktop/Bot/prediction.jpg\n"
     ]
    }
   ],
   "source": [
    "# Load recognition model\n",
    "ckpt = torch.load(REC_WT, map_location=DEVICE)\n",
    "num_cls = len(ckpt['classes'])\n",
    "\n",
    "reid = torchvision.models.resnet50(weights=None)\n",
    "reid.fc = nn.Linear(reid.fc.in_features, num_cls)\n",
    "reid.load_state_dict(ckpt['model'])\n",
    "reid = reid.eval().to(DEVICE)\n",
    "\n",
    "class_names = ckpt['classes']\n",
    "\n",
    "# Transform for recognition\n",
    "reid_tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Recognition function\n",
    "def recognise(frame_bgr):\n",
    "    \"\"\"Return list of (x1, y1, x2, y2, class_name, confidence)\"\"\"\n",
    "    results = detector.predict(frame_bgr[..., ::-1], verbose=False)  # predict expects RGB\n",
    "    out = []\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()   # <--- Correct way\n",
    "    scores = results[0].boxes.conf.cpu().numpy()  # detection confidences\n",
    "\n",
    "    for box, score in zip(boxes, scores):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        crop = frame_bgr[y1:y2, x1:x2]\n",
    "        if crop.size == 0:\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            t = reid_tf(Image.fromarray(crop[:, :, ::-1])).unsqueeze(0).to(DEVICE)\n",
    "            p = reid(t).softmax(1)\n",
    "            idx = int(p.argmax())\n",
    "            out.append((x1, y1, x2, y2, class_names[idx], float(p[0, idx])))\n",
    "    return out\n",
    "\n",
    "\n",
    "# Run a prediction\n",
    "TEST_IMG = random.choice(glob.glob(str(RAW_DIR/'cattle_*/*.jpg')))\n",
    "print('Demo image:', TEST_IMG)\n",
    "\n",
    "img = cv2.imread(TEST_IMG)\n",
    "for x1, y1, x2, y2, cname, conf in recognise(img):\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(img, f'{cname}:{conf:.2f}', (x1, y1 - 4),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "out_path = ROOT/'prediction.jpg'\n",
    "cv2.imwrite(str(out_path), img)\n",
    "print('üñºÔ∏è Annotated image saved at ‚Üí', out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from feature_extractor.pth\n",
      "‚úÖ feature extractor ready\n",
      "cosine similarity: 0.8959433436393738\n",
      "euclidean distance: 0.4561946988105774\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "from pathlib import Path\n",
    "import torch, torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "WEIGHTS = Path('feature_extractor.pth')   # path to the file you saved\n",
    "print('loading weights from', WEIGHTS)\n",
    "\n",
    "# ---------- 1. Define the *same* architecture -----------------\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, backbone_name='resnet50'):\n",
    "        super().__init__()\n",
    "        backbone = torchvision.models.__dict__[backbone_name](weights=None)\n",
    "        self.features = nn.Sequential(*list(backbone.children())[:-1])  # drop FC layer\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)            # (B, 2048, 1, 1)\n",
    "        x = x.flatten(1)                # (B, 2048)\n",
    "        return x\n",
    "\n",
    "# instantiate & load weights\n",
    "feat_extractor = FeatureExtractor()\n",
    "feat_extractor.load_state_dict(torch.load(WEIGHTS, map_location=DEVICE))\n",
    "feat_extractor.eval().to(DEVICE)\n",
    "print('‚úÖ feature extractor ready')\n",
    "\n",
    "# ---------- 2. Image transform (must be the *validation* tfm) --\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# ---------- 3. Helper to get one embedding --------------------\n",
    "@torch.inference_mode()\n",
    "def get_embedding(img_path, tfm=val_tf):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    x   = tfm(img).unsqueeze(0).to(DEVICE)          # (1,3,224,224)\n",
    "    emb = feat_extractor(x)                         # (1, 2048)\n",
    "    emb = F.normalize(emb, p=2, dim=1)              # L2-norm (optional but recommended)\n",
    "    return emb.squeeze(0).cpu()                     # ‚Üí torch tensor of shape (2048,)\n",
    "\n",
    "# ---------- 4. Similarity functions ---------------------------\n",
    "def cosine_similarity(e1, e2):\n",
    "    \"\"\"Return cos-sim in [-1,1]; higher = more similar\"\"\"\n",
    "    return float(torch.dot(e1, e2))\n",
    "\n",
    "def euclidean_dist(e1, e2):\n",
    "    return float(torch.norm(e1 - e2))\n",
    "\n",
    "# ---------- 5. Example: compare two images --------------------\n",
    "img1 = '/Users/saidheeraj/Desktop/Bot/filtered_cattle_dataset/cattle_3/IMG_20241108_092618.jpg'\n",
    "img2 = '/Users/saidheeraj/Desktop/Bot/filtered_cattle_dataset/cattle_3/IMG_20241108_092620.jpg'\n",
    "\n",
    "emb1 = get_embedding(img1)\n",
    "emb2 = get_embedding(img2)\n",
    "\n",
    "print('cosine similarity:', cosine_similarity(emb1, emb2))\n",
    "print('euclidean distance:', euclidean_dist(emb1, emb2))\n",
    "\n",
    "# ---------- 6. Example: find top-k most similar in a folder ---\n",
    "def topk_similar(query_img, gallery_dir, k=5):\n",
    "    q_emb = get_embedding(query_img)\n",
    "    scores = []\n",
    "    for p in Path(gallery_dir).glob('*.jpg'):\n",
    "        g_emb  = get_embedding(p)\n",
    "        score  = cosine_similarity(q_emb, g_emb)   # use euclidean_dist if you prefer\n",
    "        scores.append((score, str(p)))\n",
    "\n",
    "    # larger cosine similarity = more alike\n",
    "    scores.sort(reverse=True)\n",
    "    return scores[:k]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
